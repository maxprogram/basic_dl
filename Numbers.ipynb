{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwritten numbers with a Convolutional Neural Net\n",
    "\n",
    "Using Keras, Tensorflow, and the MNIST dataset to recognize handwritten digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data and graphing libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the MNIST dataset, with 60,000 samples of labeled handwritten numbers from the USPS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data('/tmp/mnist.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of the data, which is stored as an array of greyscale values, 28 by 28 pixels in size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = X_train[:3]\n",
    "for i, img in enumerate(samples):\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully connected model\n",
    "\n",
    "First, we'll create fully connected neural net without any convolution layers to see how it performs.\n",
    "\n",
    "Let's reshape the data by converting each image into a vector (array) and scale the greyscale values to 0-1 to make training the model easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape to vectors\n",
    "X_train = X_train.reshape(-1, 28*28)\n",
    "X_test = X_test.reshape(-1, 28*28)\n",
    "\n",
    "# Turn into 0-1 percentages\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255.0 # Maximum greyscale value is 255, so divide all values by 255 to get %\n",
    "X_test /= 255.0\n",
    "\n",
    "X_train[0][140:170]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have an array of 60,000 samples, each with 784 greyscale % values. But we also have to convert the labels to binary categorical data for the model to work. In this case we have 0-9 or 10 possible categories. So a 5 in categorical data would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "y_train_cat = to_categorical(y_train)\n",
    "y_test_cat = to_categorical(y_test)\n",
    "\n",
    "print(y_train[0], ' = ', y_train_cat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build the model. It will have 4 fully connected ReLU layers, ReLU making it so that the weights can't be negative. The last Softmax layer determines the 0-1 probabilities of each category and hence will have 10 perceptrons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "K.clear_session()\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_dim=28*28, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train the model and see the accuracy on our test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "fc = model.fit(X_train, y_train_cat, batch_size=128, epochs=3,\n",
    "               verbose=1, validation_split=0.3)\n",
    "\n",
    "# Test accuracy\n",
    "test_accuracy = model.evaluate(X_test, y_test_cat)[1]\n",
    "print('Fully Connected NN test accuracy = ', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we're getting accuracy of around 96% on the test data, which is pretty good -- but we can do better. By only examining and comparing each individual pixel, the model has a limited ability to find patterns in the data.\n",
    "\n",
    "## Convolutional Neural Net\n",
    "\n",
    "Now let's reshape the data again into a matrix of greyscale probabilities so that the convolution layer can try to find patterns in groups of pixels, just like humans do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1, 28, 28, 1)\n",
    "X_test = X_test.reshape(-1, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sample is now a matrix, with 28 rows and 28 columns. We can now build the model. We will keep the last 2 fully connected layers to determine the final probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPool2D, AvgPool2D\n",
    "from keras.layers import Flatten, Activation\n",
    "\n",
    "K.clear_session() # clears CPU\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(28, 28, 1)))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten()) # Flatten to feed into last layers\n",
    "# Add fully connected layers\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the summary of the model above, notice how the output shapes and parameter counts work. The first 3 layers are the convolutional network, which feed into a layer that flattens it into 5,408 nodes (13 * 13 * 32), which have 692,352 parameter weights when fed into 128 ReLU nodes (5408 * 128). \n",
    "\n",
    "Now we can train the CNN and test accuracy. (Training will take 3-5x longer due to number of params that have to be trained)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = model.fit(X_train, y_train_cat, batch_size=128,\n",
    "          epochs=3, verbose=1, validation_split=0.3)\n",
    "\n",
    "# Test accuracy\n",
    "test_accuracy = model.evaluate(X_test, y_test_cat)[1]\n",
    "print('CNN test accuracy = ', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should have seen better accuracy here, and we did. If training runs a little longer it will be even more accurate (try adjusting the number of epochs to 5-7 to see how much accuracy goes up). \n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The fully-connected, non-convolution network did surprisingly well. This is likely because the MNIST dataset is pretty simple: greyscale with only basic features to detect like lines and curves. Any more complex data with \"compounded\" features (like lines > shapes > compound features) and the FCN would not have worked.\n",
    "\n",
    "Although the CNN only did slightly better, it's the right way to go for any image data, especially if time-to-train isn't an issue."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
